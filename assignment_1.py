# -*- coding: utf-8 -*-
"""Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uwAyo5U9pniMyjW0U1vJWNC9VTDoSQQq

i) Code to upload data the California Housing dataset "*housing.csv*"
"""

#Mount the google drive and provide authentication
from google.colab import drive
drive.mount('/content/drive')

#import necessary libraries for data_preparation
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv('/content/drive/My Drive/housing.csv')
print("The first ten rows of the dataset are:")
dataset.head(10)

axes = dataset.plot.line(subplots=True,figsize=(20, 10))
plt.xlabel("Number of Instances")

# Data_cleaning drop layers having missing features.
dataset = dataset.dropna()
dataset.shape

# Y contains the feature which we intend to predict and X contains the
# remaining columns

Y = dataset['median_house_value']
X = dataset.loc[:,'longitude':'median_income']

# Split the data into training and test sets with 70% training samples and 30% test samples
x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2003)
print ("There are "+str(x_train.shape[0])+" training instances and "+str(x_test.shape[0])+" test instances")

#convert the data to numpy arrays to be used by PyTorch 
x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()
x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

"""Building the 1 Dimensional Convolution Network"""

#import the necessary libraries for PyTorch
import time
import torch
from torch.nn import Conv1d
from torch.nn import MaxPool1d
from torch.nn import Flatten
from torch.nn import Linear
from torch.nn import BatchNorm1d
from torch.nn.functional import relu #activation function
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import SGD
from torch.optim import Adam
from torch.optim import ASGD
from torch.optim import Adamax
from torch.nn import L1Loss

!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

class CnnRegressor(torch.nn.Module):
  #Intialisation method
  def __init__(self,batch_size,inputs,outputs):

    #Initialize the superclass
    super(CnnRegressor,self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_layer = Conv1d(inputs,batch_size,1)
    self.batch_norm = BatchNorm1d(8)
    #First Convolution Layer: Filters 128
    self.conv_layer = Conv1d(batch_size,128,1)
    #Second Convoluion Layer: Filters 128
    self.conv_layer_2 = Conv1d(128,128,1)
    #Third Convoluion Layer: Filters 256
    self.conv_layer_3 = Conv1d(128,256,1)
    #Max pool layer
    self.max_pooling_layer = MaxPool1d(1)
    #Flatten layer
    self.flatten_layer = Flatten()
    #Linear layer
    self.linear_layer = Linear(256,64)
    #Ouput layer
    self.output_layer = Linear(64,outputs)

  def feed(self,input):

    input = input.reshape((self.batch_size,self.inputs,1))
    output = self.batch_norm(input)
    output = relu(self.input_layer(output))
    #Three convolution layers
    output = relu(self.conv_layer(output))
    output = relu(self.conv_layer_2(output))
    output = relu(self.conv_layer_3(output))
    #max pooling layer
    output = self.max_pooling_layer(output)
    #Flatten layer
    output = self.flatten_layer(output)
    #Linear Layer
    output = self.linear_layer(output)
    #Output Layer
    output = self.output_layer(output)
    
    return output #return the output

# Build the defined model with batch size 64

batch_size = 64
model = CnnRegressor(batch_size, X.shape[1],1)
model.cuda() #to use GPU

# Function to calculate the average L1 Loss and R2 Score for every epoch.

def model_loss (model, dataset, train = False, optimizer = None):
  performance = L1Loss()
  score_metric = R2Score()
  
  avg_loss = 0
  avg_score = 0
  count = 0

  for input , output in iter(dataset):
    predictions = model.feed(input)

    loss = performance(predictions,output)

    score_metric.update([predictions,output])
    score = score_metric.compute()

    if(train):
        optimizer.zero_grad()

        loss.backward()

        optimizer.step()

    avg_loss += loss.item()
    avg_score += score
    count += 1

  return avg_loss / count, avg_score/count

# Train the model for 300 epochs 
epochs = 300

optimizer = Adam(model.parameters(),lr=0.001)

torch.cuda.synchronize()
tr_st_time = int(round(time.time()*1000))

inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train.shape[0],1)).cuda().float()

r2_score_plot = []
loss_plot = []

tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor,batch_size, shuffle=True,drop_last=True)

for epoch in range(epochs):
  avg_loss, avg_r2_score = model_loss(model,loader,train=True,optimizer=optimizer)
  r2_score_plot.append(avg_r2_score)
  loss_plot.append(avg_loss)
  print("Epoch" +str(epoch+1)+":\n\tLoss =" + str(avg_loss)+"\n\tR2_Score = " +
        str(avg_r2_score))
  
torch.cuda.synchronize()
tr_time_elapsed = int(round(time.time()*1000)) - tr_st_time
#save the trained parameters (Recommended way to save model)
torch.save(model.state_dict(), '111101_1dconv_reg.pt')

#Plot R2 Score curve and L1 Loss Curve

x_label = np.arange(1,(epochs+1))
plt.figure()
plt.xlabel("No of Epochs")
plt.ylabel("L1 Loss")
plt.plot(x_label,loss_plot)
plt.figure()
plt.xlabel("No of Epochs")
plt.ylabel("R2 Score")
plt.plot(x_label,r2_score_plot)
plt.show()

# Test  the model with the test data
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1)).cuda().float()

tensor = TensorDataset(inputs,outputs)
loader = DataLoader(tensor,batch_size, shuffle=True,drop_last=True)

torch.cuda.synchronize()
ts_start_time = int(round(time.time()*1000))
avg_loss, avg_r2_score = model_loss(model, loader)

torch.cuda.synchronize()
ts_time_elapsed = int(round(time.time()*1000)) - ts_start_time

# Print the results

print("Training time elapsed: {} ms".format(tr_time_elapsed))
print ("Test time elapsed: {} ms".format(ts_time_elapsed))

print("The model's L1 loss is: %.2f " % avg_loss)
print("The model's R^2 score is : %.6f" % avg_r2_score)
model_parameters = filter(lambda p: p.requires_grad, model.parameters())
params = sum([np.prod(p.size()) for p in model_parameters])
print("The number of trainable parameters =",params)

